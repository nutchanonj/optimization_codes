\documentclass{article}
\usepackage[utf8]{inputenc}

% some useful libraries.
\usepackage{amsmath}
\usepackage{graphicx}

% indent the first paragraph in every section.
\usepackage{indentfirst}

%--------Margin of the page------------%
\usepackage[margin=1in]{geometry}

%--------Start of Code Snipplet Setting------------%
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
%--------End of Code Snipplet Setting------------%

\begin{document}

\section*{Conjugate gradient method for finding minimizer}

\subsection*{Line search procedure}

The method is adapted directly from \emph{Numerical Optimization} by Jorge Nocedal and Stephen J. Wright, called the \emph{strong backtracking}. The procedure will be given with later explanation.
\begin{enumerate}
    \item Set $a = 1$. Set $a_{old} = 0$ then goto 2.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$ or $f(x + a s) > f(x + a_{old})$, then set $a_{low} = a_{old}$ and $a_{high} = a$ and go to step 6. Else, go to step 3.
    \item If $|s^T \nabla f(x + as)| \leq -\eta s^T \nabla f(x)$, then $\lambda = a$ and Exit. Else, go to step 4.
    \item If $s^T \nabla f(x + as) \geq 0$ then set $a_{low} = a_{old}$ and $a_{high} = a$ and go to step 6. Else, go to step 5.
    \item Let $a_{old} = a$ and $a = 2a$. Back to step 2.
    \item Set $f_{low} = f(x + a_{low}s)$. goto step 7. Note that this value will be fixed regardless of changing $a_{low}$.
    \item Use binary search or golden section search to find suitable $a$ between $a_{low}$ and $a_{high}$.
    \begin{enumerate}
        \item For binary search, let $a = (a_{low} + a_{high})/2$
        \item For golden section search, let $c = a_{low} + (a_{high} - a_{low})/\phi$ and $d = a_{high} - (a_{high} - a_{low})/\phi$, where $\phi$ is a golden ratio. If $f(x + cs) < f(x + ds)$, let $a = c$. Otherwise, let $a = d$.
    \end{enumerate}
    Then goto step 8.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$ or $f(x + a s) > f_{low}$, then let $a_{high} = a$. Then go back to step 7. Else, goto step 9.
    \item If $|s^T \nabla f(x + as)| \leq -\eta s^T \nabla f(x)$, then $\lambda = a$ and Exit. Else, go to step 10.
    \item If $(s^T \nabla f(x + as))(a_{high} - a_{low}) \geq 0$, then let $a_{high} = a_{low}$ and goto step 11.
    \item Let $a_{low} = a$ and back to step 7.
\end{enumerate}

The step-by-step explanation is as follow:
\begin{enumerate}
    \item $a$ is representing $\lambda$. Let it be $1$ first.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$, then it is violating first Wolfe's condition. It is thus ensuring that the bracket $(a_{old},a)$ will contain range that is not violating that. (The slope at $s^T \nabla f(x + a_{old}s)$ is always negative according to step 4, so that $(a_{old},a)$ must contain local minimum, that is, containing second Wolfe's point.) Also, $f(x + a s) > f(x + a_{old})$ is indicating that the function is going to increase, thus $(a_{old},a)$ must also contain local minimum (that is second Wolfe's point, too.) Note that $a_{high}$ and $a_{low}$ can swap regardless of their values.
    \item The first Wolfe's condition is already checked in step 2. Thus, if the point also satisfies second Wolfe's condition, then let it be $\lambda$ and exit.
    \item If $s^T \nabla f(x + as) \geq 0$ then it is indicating that the function is going to increase, thus $(a_{old},a)$ must contain local minimum (that is second Wolfe's point.) (It is cleared because $s^T \nabla f(x + a_{old}s)$ is always negative.) Also, note that $a_{high}$ and $a_{low}$ can swap regardless of their values.
    \item If the range $(a_{old},a)$ is not containing second Wolfe's point, slide it to the immediate right and expand it two times.
    \item This evaluated value will be used in the step 8.
    \item Finding $a$ between $a_{high}$ and $a_{low}$. $a$ will converge to range which satisfies second Wolfe's condition.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$ or $f(x + a s) > f_{low}$, then $f(x + a s)$ value is to big. Thus, let $a_{high}$ be it to lower the upper bound.
    \item The first Wolfe's condition is already checked in step 8. Thus, if the point also satisfies second Wolfe's condition, then let it be $\lambda$ and exit.
    \item $(s^T \nabla f(x + as))(a_{high} - a_{low}) \geq 0$, together with step 11, ensures us that the range between $a_{high}$ and $a_{low}$ always contain local minimum(s). To see that, let $a_{high} < a_{low}$, so $a_{high} - a_{low} < 0$. Moreover, the slope at $a_{high}$ must be negative and the slope at $a_{low}$ must be positive (from step 2, step 4, and recursive characteristics that happens here.) $a_{high} < a < a_{low}$ must be true, and in step 11 $a_{low}$ becomes $a$, so the slope at $a$ must be positive. If the slope at $a$ is negative, then, $a_{high}$ must be swap with $a_{low}$ to make the slope of $a_{high}$ positive. Then  between $a_{high}$ and $a$ will be ensured to have local minimum. The same is applied when $a_{high} > a_{low}$. Just swap the pair accordingly and we will get the same proof.
    \item Let $a_{low} = a$ to shorten the length.
\end{enumerate}

From the book \emph{Numerical Optimization}, step 2 to step 5 are called \emph{Bracket phase}, and step 7 to step 11 are called \emph{Zoom phase}.

\subsection*{Conjugate gradient method implementation}

First is an implementation of \lstinline{StrongBacktrack.m}. I choose to use the binary search because it usually give less steps than the golden section search.

\lstinputlisting[language=Matlab]{StrongBacktrack.m}

Implementation of function \lstinline{CG.m} is given here. The \lstinline{epsilon} value is used to give a threshold for norm of $x(k+1)-x(k)$. You can choose either Fletcher-Reeves or Polak-Ribiere method by indicating option (1 or 2 respectively.)

\lstinputlisting[language=Matlab]{CG.m}

Implementation of \lstinline{Rosenbrock.m} is here.

\lstinputlisting[language=Matlab]{Rosenbrock.m}

This is a script used to test the code.

\lstinputlisting[language=Matlab]{test_script.m}

The reported tabular is given here when using Fletcher-Reeves method.

\lstinputlisting{FR.txt}

The reported tabular is given here when using Polak-Ribière method.

\lstinputlisting{PR.txt}

The result indicates that Fletcher-Reeves method give a very-much faster convergence than Polak-Ribière. Polak-Ribière's uses many iterations to converge, so that many calculation of function and its gradient are needed. Furthermore, when examine the \lstinline{nReset} array, the 5 resets of Polak-Ribière's experimented in the table are only occured during the first 9 iterations of a calculation. That is, the number of resets of Polak-Ribière's is greater that that of Fletcher-Reeves's not because the number of iterations of Polak-Ribière's is much more greater, but because it is the intrinsic nature during the first few iterations of both methods themselves.

From my speculation, the reason that Polak-Ribière method converges very slow is that the minusing term, $g(k+1)-g(k)$, can be oscillated when the value of both gradients are so low that the computer precision can not handle.

\end{document}