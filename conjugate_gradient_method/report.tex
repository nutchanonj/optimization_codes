\documentclass{article}
\usepackage[utf8]{inputenc}

% some useful libraries.
\usepackage{amsmath}
\usepackage{graphicx}

% indent the first paragraph in every section.
\usepackage{indentfirst}

%--------Margin of the page------------%
\usepackage[margin=1in]{geometry}

%--------Start of Code Snipplet Setting------------%
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
%--------End of Code Snipplet Setting------------%

\begin{document}

\section*{Conjugate gradient method for finding minimizer}

\subsection*{Line search procedure}

The method is adapted directly from \emph{Numerical Optimization} by Jorge Nocedal and Stephen J. Wright, called the \emph{strong backtracking}. The procedure will be explained later.
\begin{enumerate}
    \item Set $a = 1$. Set $a_{old} = 0$, then go to 2.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$ or $f(x + a s) > f(x + a_{old})$, then set $a_{low} = a_{old}$ and $a_{high} = a$ and go to step 6. Else, go to step 3.
    \item If $|s^T \nabla f(x + as)| \leq -\eta s^T \nabla f(x)$, then $\lambda = a$ and Exit. Else, go to step 4.
    \item If $s^T \nabla f(x + as) \geq 0$ then set $a_{low} = a_{old}$ and $a_{high} = a$ and go to step 6. Else, go to step 5.
    \item Let $a_{old} = a$ and $a = 2a$. Back to step 2.
    \item Set $f_{low} = f(x + a_{low}s)$. Go to step 7. Note that this value will be fixed regardless of changing $a_{low}$.
    \item Use binary search or golden section search to find a suitable $a$ between $a_{low}$ and $a_{high}$.
    \begin{enumerate}
        \item For binary search, let $a = (a_{low} + a_{high})/2$
        \item For golden section search, let $c = a_{low} + (a_{high} - a_{low})/\phi$ and $d = a_{high} - (a_{high} - a_{low})/\phi$, where $\phi$ is a golden ratio. If $f(x + cs) < f(x + ds)$, let $a = c$. Otherwise, let $a = d$.
    \end{enumerate}
 Then go to step 8.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$ or $f(x + a s) > f_{low}$, then let $a_{high} = a$. Then go back to step 7. Else, go to step 9.
    \item If $|s^T \nabla f(x + as)| \leq -\eta s^T \nabla f(x)$, then $\lambda = a$ and Exit. Else, go to step 10.
    \item If $(s^T \nabla f(x + as))(a_{high} - a_{low}) \geq 0$, then let $a_{high} = a_{low}$ and goto step 11.
    \item Let $a_{low} = a$ and back to step 7.
\end{enumerate}

The step-by-step explanation is as follows:
\begin{enumerate}
    \item $a$ is representing $\lambda$. Let it be $1$ first.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$, then it is violating first Wolfe's condition. It is thus ensuring that the bracket $(a_{old},a)$ will contain a range that does not violate that. (The slope at $s^T \nabla f(x + a_{old}s)$ is always negative according to step 4, so that $(a_{old},a)$ must contain local minimum, that is, containing second Wolfe's point.) Also, $f(x + a s) > f(x + a_{old})$ is indicating that the function is going to increase, thus $(a_{old},a)$ must also contain local minimum (that is second Wolfe's point, too.) Note that $a_{high}$ and $a_{low}$ can swap regardless of their values.
    \item The first Wolfe's condition is already checked in step 2. Thus, if the point also satisfies the second Wolfe's condition, then let it be $\lambda$ and exit.
    \item If $s^T \nabla f(x + as) \geq 0$ then it is indicating that the function is going to increase, thus $(a_{old},a)$ must contain local minimum (that is second Wolfe's point.) (It is cleared because $s^T \nabla f(x + a_{old}s)$ is always negative.) Also, note that $a_{high}$ and $a_{low}$ can swap regardless of their values.
    \item If the range $(a_{old},a)$ does not contain the second Wolfe's point, slide it to the immediate right and expand it two times.
    \item This evaluated value will be used in step 8.
    \item Finding $a$ between $a_{high}$ and $a_{low}$. $a$ will converge to a range that satisfies the second Wolfe's condition.
    \item If $f(x + as) > f(x) + \mu a s^T \nabla f(x)$ or $f(x + a s) > f_{low}$, then $f(x + a s)$ value is to big. Thus, let $a_{high}$ be it to lower the upper bound.
    \item The first Wolfe's condition is already checked in step 8. Thus, if the point also satisfies the second Wolfe's condition, then let it be $\lambda$ and exit.
    \item $(s^T \nabla f(x + as))(a_{high} - a_{low}) \geq 0$, together with step 11, ensures us that the range between $a_{high}$ and $a_{low}$ always contain local minimum(s). To see that, let $a_{high} < a_{low}$, so $a_{high} - a_{low} < 0$. Moreover, the slope at $a_{high}$ must be negative and the slope at $a_{low}$ must be positive (from step 2, step 4, and recursive characteristics that happens here.) $a_{high} < a < a_{low}$ must be true, and in step 11 $a_{low}$ becomes $a$, so the slope at $a$ must be positive. If the slope at $a$ is negative, then $a_{high}$ must be swapped with $a_{low}$ to make the slope of $a_{high}$ positive. Then between $a_{high}$ and $a$ will be ensured to have a local minimum. The same is applied when $a_{high} > a_{low}$. Just swap the pair accordingly, and we will get the same proof.
    \item Let $a_{low} = a$ to shorten the length.
\end{enumerate}

From the book \emph{Numerical Optimization}, steps 2 to 5 are called \emph{Bracket phase}, and steps 7 to 11 are called \emph{Zoom phase}.

\subsection*{Conjugate gradient method implementation}

First is an implementation of \lstinline{StrongBacktrack.m}. I chose to use the binary search because it usually takes fewer steps than the golden section search.

\lstinputlisting[language=Matlab]{StrongBacktrack.m}

Implementation of function \lstinline{CG.m} is given here. The \lstinline{epsilon} value is used to give a threshold for the norm of $x(k+1)-x(k)$. You can choose either the Fletcher-Reeves or the Polak-Ribière method by indicating option (1 or 2, respectively).

\lstinputlisting[language=Matlab]{CG.m}

Implementation of \lstinline{Rosenbrock.m} is here.

\lstinputlisting[language=Matlab]{Rosenbrock.m}

This is a script used to test the code.

\lstinputlisting[language=Matlab]{test_script.m}

The reported table is given here when using the Fletcher-Reeves method.

\lstinputlisting{FR.txt}

The reported table is given here when using the Polak-Ribière method.

\lstinputlisting{PR.txt}

The result indicates that the Fletcher-Reeves method gives a much faster convergence than Polak-Ribière. Polak-Ribière uses many iterations to converge, so many calculations of the function and its gradient are needed. Furthermore, when examining the \lstinline{nReset} array, the 5 resets of Polak-Ribière's experiment in the table only occurred during the first 9 iterations of a calculation. That is, the number of resets of Polak-Ribière's is greater than that of Fletcher-Reeves's, not because the number of iterations of Polak-Ribière's is much greater, but because it is the intrinsic nature during the first few iterations of both methods themselves.

From my speculation, the reason that the Polak-Ribière method converges very slowly is that the minus term, $g(k+1)-g(k)$, can oscillate when the values of both gradients are so low that the computer precision can not handle them.

\end{document}